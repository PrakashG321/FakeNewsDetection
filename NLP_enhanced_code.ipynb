{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrakashG321/FakeNewsDetection/blob/main/NLP_enhanced_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLZSlfnmJaEq",
        "outputId": "6d3c8145-1c17-40e0-a9f6-e6215ed212a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Validation set performance:\n",
            "0.7357917570498915\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.67      0.69      1010\n",
            "           1       0.75      0.79      0.77      1295\n",
            "\n",
            "    accuracy                           0.74      2305\n",
            "   macro avg       0.73      0.73      0.73      2305\n",
            "weighted avg       0.73      0.74      0.73      2305\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SimpleImputer was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set performance:\n",
            "0.7251184834123223\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.68      0.68       553\n",
            "           1       0.75      0.76      0.76       713\n",
            "\n",
            "    accuracy                           0.73      1266\n",
            "   macro avg       0.72      0.72      0.72      1266\n",
            "weighted avg       0.72      0.73      0.73      1266\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Define the dataset folder path\n",
        "dataset_folder = \"/content/drive/MyDrive/liar_dataset\"  # Adjust this path as needed\n",
        "\n",
        "def read_dataset() -> tuple:\n",
        "    # these are tsv files, so put separator as '\\t'\n",
        "    train_df = pd.read_csv(f\"{dataset_folder}/train.tsv\", sep=\"\\t\")\n",
        "    test_df = pd.read_csv(f\"{dataset_folder}/test.tsv\", sep=\"\\t\")\n",
        "    valid_df = pd.read_csv(f\"{dataset_folder}/valid.tsv\", sep=\"\\t\")\n",
        "\n",
        "    # define columns\n",
        "    columns = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\",\n",
        "               \"speaker_job\", \"state_info\", \"party_affiliation\",\n",
        "               \"barely_true_counts\", \"false_counts\", \"half_true_counts\",\n",
        "               \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n",
        "\n",
        "    train_df.columns = columns\n",
        "    test_df.columns = columns\n",
        "    valid_df.columns = columns\n",
        "\n",
        "    return train_df, test_df, valid_df\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    text = str(text).lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    processed_text = \" \".join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "# Read the dataset\n",
        "train_df, test_df, valid_df = read_dataset()\n",
        "\n",
        "# Feature engineering\n",
        "def engineer_features(df):\n",
        "    df['processed_statement'] = df['statement'].apply(preprocess_text)\n",
        "    df['statement_length'] = df['statement'].apply(lambda x: len(str(x)))\n",
        "    df['speaker_job_encoded'] = pd.Categorical(df['speaker_job']).codes\n",
        "    df['party_encoded'] = pd.Categorical(df['party_affiliation']).codes\n",
        "    df['subject_encoded'] = pd.Categorical(df['subject']).codes\n",
        "    return df\n",
        "\n",
        "train_df = engineer_features(train_df)\n",
        "valid_df = engineer_features(valid_df)\n",
        "test_df = engineer_features(test_df)\n",
        "\n",
        "# Prepare features\n",
        "feature_columns = ['processed_statement', 'statement_length', 'speaker_job_encoded',\n",
        "                   'party_encoded', 'subject_encoded', 'barely_true_counts',\n",
        "                   'false_counts', 'half_true_counts', 'mostly_true_counts',\n",
        "                   'pants_on_fire_counts']\n",
        "\n",
        "# Combine train and validation sets\n",
        "combined_df = pd.concat([train_df, valid_df])\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n",
        "text_features = vectorizer.fit_transform(combined_df['processed_statement'])\n",
        "\n",
        "# Prepare other features\n",
        "other_features = combined_df[feature_columns[1:]].values\n",
        "\n",
        "# Handle NaN values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "other_features_imputed = imputer.fit_transform(other_features)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "other_features_scaled = scaler.fit_transform(other_features_imputed)\n",
        "\n",
        "# Combine all features\n",
        "X = np.hstack((text_features.toarray(), other_features_scaled))\n",
        "y = combined_df['label'].apply(lambda x: 1 if x in [\"half-true\", \"mostly-true\", \"true\"] else 0)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE for class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train Random Forest with GridSearchCV\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "print(\"Validation set performance:\")\n",
        "print(accuracy_score(y_val, y_val_pred))\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Prepare test set\n",
        "test_text_features = vectorizer.transform(test_df['processed_statement'])\n",
        "test_other_features = imputer.transform(test_df[feature_columns[1:]])\n",
        "test_other_features_scaled = scaler.transform(test_other_features)\n",
        "X_test = np.hstack((test_text_features.toarray(), test_other_features_scaled))\n",
        "y_test = test_df['label'].apply(lambda x: 1 if x in [\"half-true\", \"mostly-true\", \"true\"] else 0)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred = grid_search.predict(X_test)\n",
        "print(\"Test set performance:\")\n",
        "print(accuracy_score(y_test, y_test_pred))\n",
        "print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adrHQtxTv8sW",
        "outputId": "1e656e84-6948-4aa2-9541-a1bb31ac7df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ejZ1Qk2OxxhoSMDIgHH5wuRHFvBifByI",
      "authorship_tag": "ABX9TyPTOc/qYLGx1U715SP8vc3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}